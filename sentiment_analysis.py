# -*- coding: utf-8 -*-
"""sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aIq6jYKJhM_ZBEKhmqop5EOcoYy-hGb9

##INPUT ALL DEPENDENCIES
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

plt.style.use('ggplot')

import nltk
nltk.download('book')
from nltk.book import *
from scipy.special import softmax
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from wordcloud import WordCloud
import string
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn import preprocessing

import warnings
warnings.filterwarnings('ignore')

#Read in data
df=pd.read_csv('Ardent_Project.csv')
print(df.shape)
df.head(500)
print(df.shape)

df

df.columns

df.drop(["id","asins","keys","reviews.dateSeen","reviews.didPurchase", "reviews.id",'reviews.numHelpful',"reviews.userCity","reviews.userProvince", "reviews.username"],axis=1,inplace=True)

df.head(500)

df.count()

df.isnull().sum()

df=df.fillna(method='ffill')
df

df.isnull().sum()

df['brand'].unique()

df['categories'].unique()

df['reviews.rating'].unique()

df['reviews.text'].unique()

rev=df['reviews.text']
rev

rev_list=rev.tolist()
rev_list

df=df[~df['reviews.text'].isnull()]

df['length']=df['reviews.text'].apply(len)

"""##STEMMING"""

print( stopwords.words('english'))

corpus=[]
def clean_text(rev_list):
  rev_list=re.sub('[^a-zA-Z]',' ',rev_list) #remove special character
  rev_list=re.sub(r'\s+[a-zA-Z]\s+',' ',rev_list) #remove single character
  rev_list=re.sub(r'<.*?>',' ',rev_list)
  rev_list=rev_list.lower()
  rev_list=rev_list.split()
  ps=PorterStemmer()
  rev_list=[ps.stem(word) for word in rev_list if not word in set(stopwords.words('english'))]
  rev_list=' '.join(rev_list)
  return rev_list

df['clean_reviews']=df['reviews.text'].apply(clean_text)

print(corpus)

print(rev_list)

df.head(50)

"""##REMOVING PUNCTUATIONS"""

string.punctuation

def remove_punct(rev_list):
  text_nopunct="".join([char for char in rev_list if char not in string.punctuation])
  return text_nopunct

df['clean_reviews']=df['clean_reviews'].apply(lambda x:remove_punct(x))

df.head(10)

"""##TEXTBLOB FOR POLARITY"""

df['polarity']=df ['clean_reviews'].apply(lambda x:TextBlob(x).sentiment[0])

df.head(10)

df['polarity'].min()

"""##ANALYZING POSITIVE NEGETIVE AND NUTRAL"""

def getAnalysis(polarity):
  if(polarity<0):
    print("negative")
  elif(polarity==0):
    print("neutral")
  else:
    print("positive")

df['Textblob_Analysis']=df['polarity'].apply(getAnalysis)
df

df['polarity'].value_counts()

print(df.columns)

print(df['Textblob_Analysis'].isnull().sum())

df['Textblob_Analysis']=df['Textblob_Analysis'].astype(str)

polarity_plot=df['Textblob_Analysis'].value_counts()
print(polarity_plot)

# Function to categorize sentiment
def getAnalysis(polarity):
    if polarity < 0:
        return "negative"
    elif polarity == 0:
        return "neutral"
    else:
        return "positive"

# Apply function and store results in a new column
df['Textblob_Analysis'] = df['polarity'].apply(getAnalysis)

# Count occurrences of each sentiment
sentiment_counts = df['Textblob_Analysis'].value_counts()

# Print results
print("Sentiment Counts:\n", sentiment_counts)

# Convert to DataFrame if needed
polarity_df = sentiment_counts.to_frame().reset_index()
polarity_df.columns = ['Textblob_Analysis', 'count']
print(polarity_df)

import matplotlib.pyplot as plt

# Plot the sentiment distribution
plt.figure(figsize=(8, 5))
sentiment_counts.plot(kind='bar', color=['green', 'red', 'blue'])
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.title("Sentiment Analysis of Reviews")
plt.xticks(rotation=0)
plt.show()

# Pie chart for sentiment distribution
plt.figure(figsize=(6, 6))
sentiment_counts.plot(kind='pie', autopct='%1.1f%%', colors=['green', 'red', 'blue'])
plt.ylabel("")
plt.title("Sentiment Analysis Breakdown")
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Convert text data into numerical features
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_reviews'])
y = df['Textblob_Analysis']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression Model
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Convert text data into numerical features using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # Convert reviews to feature vectors
X = vectorizer.fit_transform(df['clean_reviews'])

# Convert sentiment labels into numeric values (0=Negative, 1=Neutral, 2=Positive)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Textblob_Analysis'])

# Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data size:", X_train.shape)
print("Testing data size:", X_test.shape)

from sklearn.naive_bayes import MultinomialNB

# Initialize and train the model
model = MultinomialNB()
model.fit(X_train, y_train)

print("Multinomial NaÃ¯ve Bayes model training complete!")

from sklearn.metrics import classification_report, accuracy_score

# Predict sentiment on test data
y_pred = model.predict(X_test)

# Print model accuracy and classification report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Convert text data into numerical features using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # Feature extraction
X = vectorizer.fit_transform(df['clean_reviews'])

# Convert sentiment labels into numerical categories (0=Negative, 1=Neutral, 2=Positive)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Textblob_Analysis'])

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data size:", X_train.shape)
print("Testing data size:", X_test.shape)

from sklearn.tree import DecisionTreeClassifier

# Initialize the Decision Tree model
model = DecisionTreeClassifier(criterion="entropy", max_depth=20, random_state=42)

# Train the model
model.fit(X_train, y_train)

print("Decision Tree model training complete!")

from sklearn.metrics import classification_report, accuracy_score

# Predict sentiment on test data
y_pred = model.predict(X_test)

# Print accuracy and classification report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

import joblib

# Save trained model
joblib.dump(model, 'decision_tree_sentiment.pkl')

# Save vectorizer
joblib.dump(vectorizer, 'vectorizer.pkl')

# Save label encoder
joblib.dump(label_encoder, 'label_encoder.pkl')

print("Model and vectorizer saved successfully!")

# Load saved model, vectorizer, and label encoder
model = joblib.load('decision_tree_sentiment.pkl')
vectorizer = joblib.load('vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')

# Function to predict sentiment
def predict_sentiment(new_text):
    new_text = [new_text]  # Convert to list
    transformed_text = vectorizer.transform(new_text)  # Convert text to TF-IDF features
    prediction = model.predict(transformed_text)  # Predict sentiment
    return label_encoder.inverse_transform(prediction)[0]  # Convert label back to sentiment

# Example: Test on a new review
new_review = "The service was very slow, I'm disappointed."
print("Predicted Sentiment:", predict_sentiment(new_review))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Convert text data into numerical features using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # Feature extraction
X = vectorizer.fit_transform(df['clean_reviews'])

# Convert sentiment labels into numerical categories (0=Negative, 1=Neutral, 2=Positive)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Textblob_Analysis'])

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data size:", X_train.shape)
print("Testing data size:", X_test.shape)

from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
model = RandomForestClassifier(n_estimators=100, criterion="entropy", max_depth=20, random_state=42)

# Train the model
model.fit(X_train, y_train)

print("Random Forest model training complete!")

from sklearn.metrics import classification_report, accuracy_score

# Predict sentiment on test data
y_pred = model.predict(X_test)

# Print accuracy and classification report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

